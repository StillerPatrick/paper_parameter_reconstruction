\documentclass[hyperref,german,beleg]{cgvpub}
\usepackage{subfigure}
%weitere Optionen zum Ergänzen (in eckigen Klammern):
% 
% female	weibliche Titelbezeichnung bei Diplom
% bibnum	numerische Literaturschlüssel
% final 	für Abgabe	
% lof			Abbildungsverzeichis
% lot			Tabellenverzeichnis
% noproblem	keine Aufgabenstellung
% notoc			kein Inhaltsverzeichnis
% twoside		zweiseitig
\author{Patrick Stiller}
\title{Parameterrekonstruktion für Röntgen-Kleinwinkelstreuung mit Deep Learning}
\birthday{8. Januar 1994}
\placeofbirth{Dresden}
\matno{3951290}
\betreuer{Dr. Dmitrij Schlesinger, Dr. Heide Meißner, Dr. Michael Bussmann}
\bibfiles{literatur}
\problem{aufgabenstellung}
\copyrighterklaerung{Hier soll jeder Autor die von ihm eingeholten
Zustimmungen der Copyright-Besitzer angeben bzw. die in Web Press
Rooms angegebenen generellen Konditionen seiner Text- und
Bild"ubernahmen zitieren.}
\acknowledgments{Die Danksagung...}
\begin{document}

\chapter{Einleitung}

\chapter{Grundlagen}

\section{Physikalischer Hintergrund}

\subsection{Kleinwinkelstreuung}
Die Kleinwinkelstreuung in Englisch Small Angle X-Ray Scattering (SAXS) ist eine universelle Technik zur Untersuchung von Feststoffen. Dabei kann SAXS Informationen über die kristalline Struktur, chemische Komposition und die physikalische Eigenschaften eines untersuchten Feststoffes liefern \cite{THEORYSAXS}. Die Untersuchung von Feststoffen unter Einfluss von Kurzpulslasern ist eine wichtige Aufgabe der heutigen Physik. Wissen über das Verhalten von Feststoffen unter Einfluss von Kurzpulslasern kann offene Fragen in der Krebsforschung und in der Astrophysik beantworten. Dieser Beleg bezieht sich auf die Publikation von Thomas Kluge, welches 2018 mit dem Titel:\grqq Observation of ultrafast solid-density plasma dynamics using femtosecond X-ray pulses from a free-electron laser\grqq veröffentlicht wurde. In der angesprochenen Publikation wurde SAXS für die Untersuchung von Plasma eingesetzt \cite{SAXS18}. 

\begin{figure}[ht]
	\centering
		\includegraphics [scale=0.5]{images/saxs_setup.png}
	\caption{Schematischer Aufbau des Experimentaufbaus. Im rechten Teil der Abbildung sind Detektorbilder in Abhängigkeit des Einsetzten des   		Röntgenlaserpulses dargestellt. Bildquelle: \cite{SAXS18}}
	\label{fig:saxssetup}
\end{figure}

\subsection{Experimentbeschreibung}

Das in \cite{SAXS18} beschriebene Experiment besteht aus vier Hauptkomponenten (siehe Abbildung \ref{fig:saxssetup} links). Dem Target, dem Hochintensitätslaser (UHI), dem Röntgen-Freie-Elektronen-Laser(XFEL) und dem Detektor.  Beim Target handelt es sich um ein Kupfertarget. In das Kupfertarget wurde eine Gitterstruktur eingraviert, um eine analytische Beschreibung des Experimentausgangs zu ermöglichen.  Zusätzlich wird das Taget mit einer 2 $\mu$m breiten Siliziumschicht überzogen. Durch die eingravierte Gitterstruktur besitzt das Target nur noch einen eindimensionalen Informationsgehalt und kann somit durch drei Parameter vollständig beschrieben werden. Die drei Paramter sind Pitch, Feature-Size und die Aufweichungsbreite $ \sigma $. Dabei beschreiben Pitch und Feature-Size die Struktur des Targets und Sigma den Aufweichungseffekt des Hochintensitätslasers (siehe Abbildung \ref{fig:grating_structure}).

\begin{figure}[hh]
	\centering
		\includegraphics [scale=0.5]{images/grating_structure.png}
	\caption{Analytische Beschreibung des Querschnittes des Targets. Dabei ist das Grating zu erkennen und dass es durch drei Parameter: Pitch, Feature-Size und Sigma beschrieben werden kann. Bildquelle: \cite{Zach17}}
	
	\label{fig:grating_structure}
\end{figure}
Während des Experiments wird das Target durch einen Hochintensitätslaser in einem Winkel von 90° beschossen. Augrund des Elektrischen Feldes des Lasers entsteht Plasma. Bei Plasma handelt es sich um ein Gemisch aus freien Elektronen, positiven Ionen und neutralen Teilchen, welche unter ständiger Wechselwirkung untereinander und mit Photonen stehen. Dadurch kann es zu unterschiedlichen Energie- bzw. Anregungszuständen kommen. Der Plasmazustand eines Stoffes wird auch als vierter Aggregatzustand bezeichnet \cite{PLASMADEF}. Während des Beschusses durch den Hochintensitätslaser ist außerdem ein Schmelzprozess und somit eine Aufweichung der Gitterstruktur des Targets wahrzunehmen (siehe Abbildung \ref{fig:melting_grating}). 

\begin{figure}[hh]
\centering
\includegraphics [scale=0.7]{images/melting_grating.png}
\caption{Veränderung des Gratings nach einer Bestrahlungsdauer von 60 Femtosekunden Bildquelle: \cite{SAXS18}}
\label{fig:melting_grating}
\end{figure}

Das durch den Hochintensitätslaser erzeugte Plasma soll es auf seine Struktur und Elektrodynamik untersucht werden. Dafür wird leicht zeitversetzt ein zweiter Laser benutzt. Dabei handelt es sich um einen Röntgen-Freie-Elektronen-Laser, welcher in einem Winkel von 45° mit einer Pulsdauer von 40 Femtosekunden auf das Target schießt. Bei diesem Vorgang kommt es zu einem Streuvorgang des Lichts des Röntgenlasers an den Elektronen des Targets. Die Intensitäten des gestreuten Röntgenlasers werden durch einen Detektor, welches hinter dem Target platziert ist, gemessen(Abbildung \ref{fig:saxssetup} links). Beim Detektor handelt es sich um ein Raster von Lichtdetektoren, welche die ankommenden Lichtintensitäten messen. Dabei ist der Lichtdetektor Zeit integrierend, dass heißt, das ankommende Lichtintensitäten über die Zeit summiert werden. Diesen Effekt ist in der Abbildung \ref{fig:electron_scattering} zu erkennen, welche einen beispielhaften Streuprozess an zwei Elektronen zeigt. 

\begin{figure}[hh]
	\centering
		\includegraphics [scale=0.4 ]{images/electron_scattering.png}
	\caption{Beispielhafte Darstellung einer Streuung an zwei Elektronen. Die Röngenlaserpulse treffen geradlinig auf die Elektronen und werden dann bei den Elektronen gestreut. Der Detektor dahinter misst die ankommenden Intensitäten der kreiförmigen Wellen und summiert diese über die Zeit.  Bildquelle: \cite{Zach17}}
\label{fig:electron_scattering}
\end{figure}

Durch die Zeitintegrität des Detektors, gehen die zeitlichen Abstände der Wellen verloren (Phase). Das entstandene Detektorsignal ist äquivalent zum Betragsquadrat der Fouriertransformation Gitterstruktur $\eta $ (siehe Gleichung \eqref{eq:xrayscattering}) \cite{SAXS18}.


\begin{equation}\label{eq:xrayscattering}
\Phi \approx | r _{0} \cdot \int \eta (\vec r) \cdot e^{i\vec q \vec r} d \vec r|^2
\end{equation}



\section{Problemidentifikation }
Einer der Herausforderungen des beschriebenen Experiment ist die Zeitintegrität des Detektors. Diese Detektoreigenschaft ruft den Verlust der Phase hervor, sodass eine Rekonstruktion der Elektronendichteverteilung $ \eta $ mit Hilfe einer inversen Fouriertransformation (IFT) nicht möglich ist. Um dieses Problem zu lösen werden iterative Algorithmen, sogenannte Phaseretrieval-Algorithmen verwendet. Beispiele für solche Algorithmen sind \cite{Fienup:82}: Error-Reduction Algorithm, Gradient Search Methods und der Input Output Algorithmus. Probleme bei diesen Algorithmen sind, dass erstens bei allen Algorithmen keine Konvergenz garantiert ist und zweitens sehr viele Iterationen notwendig sind um eine Optimierung der errechneten Phase zu erzeugen. Diese Eigenschaften führen dazu, dass eine hochfrequente Verarbeitung von Experiment Ergebnissen verwehrt bleibt. Deswegen soll mit Hilfe von Deep Learning die Phase bzw. die Eigenschaften (drei Parameter zur Beschreibung des Gratings) der Elektronendichteverteilung $ \eta $ rekonstruiert werden. Der gewählte Deep Learning Ansatz ist ein neuronales Netz. Ein neuronales Netz hat den Vorteil, dass es sich im Gegensatz zu iterativen Verfahren durch GPU parallelisieren lässt und es die Phaseninformation bzw. die Parameter der Elektronendichte nicht-iterativ bestimmt(Einschrittverfahren) . 

\section{Fouriertransformation}
Die Fouriertransformation (benannt nach Jean Baptiste Joseph Fourier) ist eine Transformation, welche zeitbezogene Welle im Ortsraum in ihre frequenzmä\ss igen Spektralanteile zerlegt. Bei der Fouriertransformation wird die Welle in Teilwellen zerlegt (Abbildung \ref{fig:fourier_example}) und wird somit als Transformation vom Orts- in den Frequenzraum bezeichnet. Die Fouriertransformation ist folgendermaßen definiert:

\begin{equation}\label{eq:fourier-series}
F ( u ) = \int _ { - \infty } ^ { \infty } f ( x ) e ^ { - 2 \pi i x u } d x
\end{equation}

die Fourier-Koeffizienten $\gamma_{k}$  werden wie bei einem Basiswechsel durch ein Integral bestimmt( Gleichung \eqref{eq:fourier_coefficients}). Dabei ist das Ergebnis des Integrals die Länge der Projektion in Richtung der Basis des Frequenzraumes. 

\begin{equation}\label{eq:fourier_coefficients}
\gamma _ { \mathrm { k } } = \frac { 1 } { T } \int _ { 0 } ^ { \top } f _ { \mathrm { T } } ( \tau ) e ^ { - i k \omega \tau } \mathrm { d } \tau
\end{equation}

Somit ergibt sich als Resultat der Fouriertransformation eine Folge von Komplexenzahlen $u$ mit zugehörigen Koeffizienten $ \gamma_{k}$. Jede Komplexe Zahl kodiert eine Welle, welche über die Euler Identität $e ^ { i k x } = \cos ( k x ) + i \cdot \sin ( k x )$ errechnet werden kann. Zusätzlich kann noch das Amplituden- und das Phasenspektrum bestimmt werden.  Dabei bestimmt dieAmplitude das Maxima und das Minima der zugehörigen Welle und die Phase die Verschiebung der Welle. Das Amplitudenspektrum $| F ( u ) | $ und das Phasenspektrum $\phi ( u )$ ergeben sich durch die folgende Gleichungen \cite{FOURIER2}.

\begin{equation}
| F ( u ) | = \sqrt { R ^ { 2 } ( u ) + I ^ { 2 } ( u ) }
\end{equation}

\begin{equation}
\phi ( u ) = \tan ^ { - 1 } \frac { I ( u ) } { R ( u ) }
\end{equation}

\begin{figure}[hh]
	\centering
		\includegraphics [scale=0.5]{images/example_fourier.png}
	\caption{Überlagerung mehrerer Wellen um eine Rechteckfunktion zu approximieren. Bildquelle: \cite{Gallagher2008AnIT}}
	\label{fig:fourier_example}
\end{figure}


\section{Filter}

Ein Filter ist eine Operation, welche auf einem Signal verwendet wird um Signale zu glätten, Signalstörungen zu vermeiden oder um Rauschen zu verhindern. In den meisten Fällen wird die Filteroperation mit Hilfe von Faltung realisiert. Dabei wird die Faltung zweier Funktionen (f * g) durch den  funktionalen Zusammenhang in Formel \eqref{eq:convolution} beschrieben. Dabei ist f die Funktion, welche das Signal beschreibt und g die Funktion, welche den Filter beschreibt. Im diskreten Fall wird das Integral durch eine Summe bis zur entsprechenden Filtergröße ersetzt \cite{CONV}. 

\begin{equation}\label{eq:convolution}
( f * g ) ( x ) : = \int _ { \mathbb { R } ^ { n } } f ( \tau ) g ( x - \tau ) \mathrm { d } \tau
\end{equation}



\section{Neuronale Netze}
\subsection{Aufbau}
Neuronale Netze sind eine mathematische Adaption des realen menschlichen Gehirns. Ein neuronales Netz besteht aus vielen kleinen Komponenten, Neuronen, welche durch gerichtete und gewichtete Verbindungen verbunden sind.  Mathematisch definiert ist ein neuronales Netz ein Tripel (N,V,w) mit den Beiden Mengen N und V und der Funktion w.  N ist die Menge aller Neuronen und V = $\{ (i,j) | i, j \epsilon \mathbb{N}\}$  die Menge der  Verbindungen zwischen Neuron i und Neuron j . Die Funktion $w : V \longrightarrow \mathbb{R} $  beschreibt die Gewichte des Neuronalen Netzes. Wobei w(i,j) das Gewicht zwischen dem Neuron i und Neuron j beschreibt. Im Allgemeinen wird anstatt der Funktionsnotation die Notation $w_{i,j}$ für die Gewichte zwischen zwei Neuronen verwendet.  \cite{Kriesel2007NeuralNetworks} Die nächste wichtige Komponente ist die Propagierungsfunktion $net_{j}$  eines Neurons, welche einen wichtigen Teil des Informationsflusses in einem Neuronalen Netz definiert. Dabei wird der Input des Neurons j durch dessen Propagierungsfunktion $net_{j}$ bestimmt. Die Propagierungsfunktion $net_{j}$ nimmt den Output aller Neuronen welche eine ausgehende Verbindung zum Neuron j besitzen als Input.  So wird die Propagierungsfunktion $net_{j}$ durch folgenden funktionalen Zusammenhang beschrieben : 

\begin{equation}\label{eq:propagation_function}
 net_ { j } = \sum _ { i \in I_{j}} ( o _ { i }\cdot w _ { i , j }) \textrm{ mit } I_{j} = \{ i \epsilon N | (i,j) \epsilon V \}
\end{equation}

Der Output  $o_{j} $ eines Neuronen j wird mit Hilfe der Aktivierungsfunktion $a_{j}$ und der der Propagierungsfunktion berechnet. Dazu wird noch der Schwellwert $ \theta_{j} $ zur Hemmung des Aktivierungszustand des Neurons zur Hilfe genommen. Somit ergibt sich für den Output des Neurons folgender funktionaler Zusammenhang : 

\begin{equation}
o_{j} = a_{j}(net_{j} - \theta_{j}) = f(x)
\end{equation}

Die Wahl der Aktivierungsfunktion ist von Anwendungsfall von Anwendungsfall unterschiedlich. In den meisten Fällen werden differenzierbare Aktivierungsfunktionen benutzt, da sie den Lernprozess, des neuronalen Netzes erleichtern. Das erste Beispiel für eine Aktivierungsfunktion ist die Heaviside-Funktion(Step-Funktion), welche auf 0 oder 1 abbildet(Abbildung \ref{fig:activations}a). Diese Funktion ist an der Stelle 0 nicht differenzierbar und im generellen für moderne Optimierungsansätze für neuronale Netze nicht geeignet, da die Ableitung der Step-Funktion an allen Stellen 0 ist. Beispiel für gängige und differenzierbare Funktionen sind die Sigmoid-(Abbildung \ref{fig:activations}b, Formel \eqref{eq:sigmoid}), Tangens Hyperbolicus-(Abbildung \ref{fig:activations}c, Formel \eqref{eq:tanh}) und die Rectified Linear Units- Aktivierungsfunktion(Abbildung \ref{fig:activations}d, Formel \eqref{eq:relu})

\begin{equation}\label{eq:sigmoid}
f(x)= \frac { 1 } { 1 + e ^ { - x } }
\end{equation}

\begin{equation}\label{eq:tanh}
f(x)= tanh(x) = 1 - \frac { 2 } { \mathrm { e } ^ { 2 x } + 1 }
\end{equation}

\begin{equation}\label{eq:relu}
f(x)= max(0,x)
\end{equation}

\begin{figure}[hh]
	\subfigure[Heavisidefunction]{\includegraphics[width=0.5\textwidth]{images/step.png}}
    \subfigure[Sigmoid]{\includegraphics[width=0.5\textwidth]{images/sigmoid.png}} 
     \subfigure[Tanh]{\includegraphics[width=0.5\textwidth]{images/tanh.png}} 
    \subfigure[Relu]{\includegraphics[width=0.5\textwidth]{images/relu.png}} 
    \caption{Übersicht Aktivierungsfunktionen, Bildquelle:  \cite{Kriesel2007NeuralNetworks}}
\label{fig:activations}
\end{figure} 

Jeder der genannten Aktivierungsfunktion hat auf Hinsicht seiner Berechnungsdauer, Wertebereich und Ableitung Vor- und Nachteile. Die Sigmoid und die Tangens Hyperbolicus  Aktivierungsfunktion haben einen eingeschränkten Wertebereich, dadurch kann es mit diesen Aktivierungsfunktionen zu keinen zu hohen Werten im Neuronalen Netz kommen und negative Werte werden im Gegensatz zu ReLu noch berücksichtigt. Jedoch sind beide langsamer zu berechnen als die Rectified Linear Units und bieten für den Lernprozess kleinere Gradienten. Rectified Linear Units(ReLu) ist im Vergleich zu Sigmoid und Tangens Hyperbolicus schneller zu berechnen und bietet größere Gradienten. Jedoch können Neuronen, welche einen negativen Input bekommen nur noch 0 Ausgeben. Man spricht in diesem Zusammenhang von einem totem Neuron. Ein weiterer Nachteil von ReLu ist der Wertebereich,denn dieser ist nicht eingeschränkt. Somit können im Neuronalen Netz sehr große Werte entstehen, welche den Wertebereich von Zahlenstandards wie zum Beispiel 32 Bit Float überschreiten und somit kein valider Datenfluss im Neuronalen Netz gegeben ist. 

\subsection{Feed-Forward-Neuronal  Networks}
Für ein Neuronales Netzwerk werden verschiedene Netzwerktopologien verwendet. Ein Beispiel dafür ist das Feed Forward Neuronal Network (Abbildung \ref{fig:ffnn}) Bei einem Feed Forward Neuronal Network werden die Neuronen als Schichten angeordnet. Diese Schichten sind miteinander verbunden. Dabei ist die Erste Schicht, welche die Input Daten bekommt, wird als Input-Layer bezeichnet. Und die letzte Schicht, welche die Netzberechnung ausgibt, als Output-Layer bezeichnet. Schichten, welche sich zwischen Input- und Output-Layer befinden und somit keinen Kontakt nach Außen haben, werden als Hidden Layer bezeichnet. Ein wichtiges Merkmal von Feed Forward Networks ist, dass der Datenfluss geradlinig ohne Rückkopplung von Input-Layer über die Hiddenlayer bishin zum Output-Layer verläuft. 

\begin{figure}[hh]
\centering
\includegraphics [scale=0.4]{images/feed_forward_neuronal_network.png}
\caption{Beispielhafte Darstellung für ein Feed-Forward-Neuronal-Network. Bildquelle: \cite{COMPMETHODS}}
\label{fig:ffnn}
\end{figure}

\subsection{Lernprozess}
Das Anlernen von Neuronalen Netzen wird in den meisten Fällen durch überwachtes Lernen (supervised Learning) realisiert. Im speziellen wird der Lernprozess durch den Backpropagation Algorithmus durchgeführt. Bei einem überwachten Lernansatz besteht der Datensatz zum Trainieren des Neuronalen Netzen aus zwei Teilen. Zu jedem Netzinput $x_{i}$ gibt es ein zugehöriges Label $y_{i}$ , welches den gewünschten Netzoutput definiert. Der Lernprozess eines neuronalen Netzes kann in drei Phasen aufgeteilt werden. Der Forward-Pass, Loss-Calculation und Backward Pass \cite{NeuronaleNetze}. Zu Beginn des Trainingsprozesses werden die Gewichte des Neuronalen Netzes mit Zufallszahlen initialisiert. Beim Forward Pass werden die Input Daten zur Kalkulation des derzeitigen Net-Outputs zum Input-Layer des neuronalen Netzes gegeben. Das Neuronale Netz bestimmt dann durch die Rechenvorschriften des Neuronalen Netzes den Net-Output $\hat { y } _ { i }$. Im zweiten Schritt wird dann die Qualität des Netouput $\hat { y } _ { i }$ bestimmt.. Dazu wird ein Fehlermaß benutzt, was den Grad des Unterschiedes zwischen $\hat { y } _ { i }$ und $y_{i}$  bestimmt. Eine beispielhafte Errorfunktion ist Mean-Squared Error \eqref{eq:mse}. 


\begin{equation}\label{eq:mse}
\mathrm { MSE } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( Y _ { i } - \hat { Y } _ { i } \right) ^ { 2 }
\end{equation}

Im drittem Schritt, dem Backward-Pass, kommt es zur Optimierung des Neuronalen Netzes. Mithilfe des Fehlers, welcher bis zur Eingabeschicht zurück propagiert, werden die Gewichte, je nach ihrem Einfluss auf den Net-Output angepasst. Dazu wird ein Gradientenabstiegsverfahren verwendet. Dafür wird die partielle Ableitung des Fehlerterms benötigt. Somit ergibt sich für die Gewichtsveränderung $\Delta w _ { i j }$ des Gewichts zwischen Neuron i und Neuron j durch folgenden Funktionalen Zusammenhang : 

\begin{equation}\label{eq: deltaw}
\Delta w _ { i j } = - \eta \frac { \partial E } { \partial w _ { i j } } = - \eta \delta _ { j } o _ { i }
\end{equation}

Dabei ist E die Errorfunktion. $\delta _ { j }$ das Fehlersignal des bzw. der Gradient Neurons j und $o _ { i }$ die Ausgabe des Neuron i. Der letzte wichtige Parameter ist $\eta $ welche die Learning-Rate des Gradientenabstiegsverfahren bestimmt. Schlussendlich fehlt die Definition des Gradienten. Dieser ist davon abhängig, wie stark das Neuron den Output des Neuronalen Netz beeinträchtigt. Deswegen wird eine Unterscheidung getroffen, ob das Neuron sich im Output-Layer oder darunter befindet. In der folgenden Gleichung \eqref{eq:gradient} ist die Definition des Gradienten und die zugehörige Propagierung des Fehlers. 


\begin{equation}\label{eq:gradient}
\delta _ { j } = \left\{ \begin{array} { l l } { a_{j}  \left( \text { net } _ { j } \right) \left( o _ { j } - t _ { j } \right) } & { \text { falls } j \text { Ausgabeneuron ist } } \\ { a_{j}  \left( \text { net } _ { j } \right) \sum _ { k } \delta _ { k } w _ { j k } } & { \text { falls } j \text { verdecktes Neuron ist. } } \end{array} \right.
\end{equation}

Dabei ist die Gleichung \eqref{eq:gradient} in Abhängigkeit von den Variablen $o_{j}$ dem Output des Neurons j , $t_{j}$ die Soll-Ausgabe des Neurons j und der Aktivierungsfunktion $a_{j} $ des Neuron j angegeben. Und somit ergibt sich die Veränderung des neuem Gewicht durch eine Addition des alten Gewichts. 

\begin{equation}
w _ { i j } ^ { \text { neu } } = w _ { i j } ^ { \text { alt } } + \Delta w _ { i j }
\end{equation} 

Der Backpropagation wird iterativ solange angewandt, bis eine bestimmte Anzahl von Iterationen erreicht ist, oder andere Kriterien erfüllt sind.  \cite{wiki:Backpropagation}


\chapter{Simulation}
\section{Motivation}
Um das Ergebniss, des SAXS-Experimentes bestmöglich zu verstehen und aufgrund einer mangelnder Anzahl von Experimentdaten, wurde im Rahmen der Master Arbeit von Malte Zacharias mit dem Titel: \grqq Model-Driven Parameter Reconstruction from Small Angle X-Ray Scattering Images \grqq eine Modellierung entwickelt. Die Modellierung beinhaltet das Design der Gitterstruktur (Elektronendichteverteilung), die Modellierung des Hochenergielasereinflusses,die Modellierung des Streuvorgangs und die Modellierung des Detektorbildes. Das ganze Kapitel bezieht sich auf die Master Arbeit von Malte Zacharias \cite{Zach17}. 

\section{Simulationsbeschreibung}
\subsection{Design der Gitterstruktur und Simulation des Hochenergielasereinflusses}
Wie in Kapitel 2 beschrieben sind die Targets des SAXS-Experiments als Gitter strukturiert, um eine analytische Beschreibung des Targets zu ermöglichen. Der erste Simulationsschritt ist die Bestimmung der Gitterstruktur. Die Breite des Gratings ist auf N Pixel beschränkt. Die Struktur des Gratings ist über die drei Startparameter Pitch, Feature-Size und Sigma definiert. Dabei ist der Parameter Feature-Size für die Breite eines Features zuständig und der Parameter Pitch für die Periodizität, somit sind beide Parameter für die Struktur des Gratings ohne Einfluss des Hochenergielasers verantwortlich. Der Parameter Sigma bestimmt die Aufweichungsbreite des Gratings und somit den Einfluss des Hochenenergielasers. Zunächst wird anhand der Parameter Feature-Size und Sigma, die Struktur eines Features bestimmt.  Dazu wird eine Rechteckfunktion (\( 1_{[0,\text{Feature-Size}]} \)),welche die Breite eines Features bestimmt, mit einer Gaussverteilung (\(\exp \left( - x ^ { 2 } / 2 \sigma ^ { 2 } \right) \)) gefaltet \cite{SAXS18}, um den Hochenergielasereinfluss zu simulieren. Das Ergebnis der Faltung wird mit Hilfe der Errorfunktion (Gleichung \eqref{eq:erf}) dargestellt. 

\begin{equation}\label{eq:erf}
\text{erf(x)} = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-\xi^{2}}d\xi
\end{equation}

Das Ergebnis der Faltung des Rechteckimpulses und der Gauss-Verteilung ist die Modellierung eines Features:

\begin{equation}\label{eq:feature}
	\tilde{\eta} = \frac{\sqrt{\pi}\sigma}{2}(\text{erf}(\frac{x}{\sqrt{2}\sigma}) - \text{erf}(\frac{x - \text{fsize}}{\sqrt{2}\sigma}))
\end{equation}

Das modellierte Feature wird durch eine weitere Faltung mit mehreren um Pitch verschobene Dirac-Impulsen \cite{beucher_2011} periodisch fortgesetzt wird, um die Grating-Struktur zu komplettieren. 


\subsection{Effekt der Startparameter auf die Gitterstruktur}

Die drei Startparameter Sigma (\( \sigma\)), Pitch  und Feature-Size(fsize), welche die Struktur des Gratings beschreiben, haben unterschiedliche Effekte auf die Gratingstruktur. Sigma ist der Parameter, welcher, wie bereits beschrieben, den Einfluss des Hochenergielasers auf ein Feature modelliert. Desto höher Sigma gewählt wird, desto mehr wird die Kantenstruktur des Targets aufgeweicht. In Abbildung \ref{img:edge_sigma} ist dieser Effekt dargestellt. 

\begin{figure}[hh]
\centering
\includegraphics[scale=0.30]{images/edge_sigma.png} 
\caption{Vergleich der Kantenstruktur unter variierenden Sigma-Werten bei gleichbleibenden Pitch- und Feature-Size- Werten}
\label{img:edge_sigma}
\end{figure}

Der Parameter Pitch(p) bestimmt die Periodizität der Kantenstruktur.  Daraus folgt, dass mit steigendem Pitch-Wert die Anzahl der Features sinkt. Dieser Effekt ist in der Abbildung \ref{img:edge_pitch_fsize} in der rechten Spalte dargestellt. Der dritte Startparamter Feature-Size(f) bestimmt die Größe der Features. Dabei kann die Feature-Size nicht größer als Pitch gewählt werden, da sonst keine Kantenstruktur mehr erkennbar wäre. Der Effekt der Feature-Size wird in Abbildung \ref{img:edge_pitch_fsize} in der linken Spalte dargestellt.


\begin{figure}[hh]
	\centering 
	\includegraphics[scale=0.35]{images/pitch_fsize_edge.png} 
	\caption{Vergleich der Kantenstruktur unter variierenden Pitch-Werten(rechte Spalte) und variierenden Feature-Size-Werten(linke Spalte)} 
	\label{img:edge_pitch_fsize}
\end{figure} 



\subsection{Simulation der Streuung}

Wie im Kapitel zur Experimentbeschreibung beschrieben, ist das aufgenommene Detektorbild äquivalent zum Betragsquadrat der  Fouriertransformation der Elektronendichte $\eta $.  Im Fall der Simulation ist die Elektronendichte eine diskrete eindimensionale Elektronendichteverteilung. Deswegen wird zur Bildung des Detektorbildes eine eindimensionale diskrekte Fouriertransformation  \eqref{eq:fft} verwendet.

\begin{equation}\label{eq:fft}
\hat { a } _ { k } = \sum _ { j = 0 } ^ { N - 1 } e ^ { - 2 \pi \mathrm { i } \cdot \frac { j k } { N } } \cdot a _ { j } \text { für } k = 0 , \ldots , N - 1
\end{equation}

Das entstandene Produkt der Fouriertransformation ist im Raum der Komplexen Zahlen und weißt den gleichen Informationsgehalt wie die Elektronendichteverteilung auf. Das heißt Amplituden- und Phaseninformationen sind nach wie vor vorhanden. Wie in Kapitel 1 beschrieben, ist der Detektor zeitintegrierend und die Phase geht verloren, deswegen wird die Phaseninformation im weiteren Simulationsverlauf nicht weiter verwendet. Um schlussendlich die aufgenommenen Intensitäten des Detektors (siehe Abbildung \ref{img:detector}) zu erhalten, wird das Betragsquadrat des Amplitudenspektrums gebildet. 

\begin{figure}[hh]
\centering
\includegraphics[scale=0.3]{images/endproduct.png} 
\caption{Resultierendes Detektorbild als Endprodukt der Simulation}
\label{img:detector}
\end{figure}

\subsection{Auswirkung der Startparameter auf das Simulationsergebnis}
Die Startparameter des Simulation haben ebenfalls Auswirkung auf das Endprodukt der Simulation. Betrachtet man zunächst die Auswirkungen des Startparameters Pitch (Abbildung \ref{img:pitch_detector}), kann man erkennen, dass mit steigendem Pitch sich das Intensitätsmaximum verringert. Zusätzlich kommen noch mehr Peaks hinzu. Das hängt damit zusammen, dass mit steigenden Pitch sich die Anzahl der Features verringert und somit mehr Frequenzen benötigt werden, um die Gitterstruktur zu modellieren. 

\begin{figure}[hh]
\centering
\includegraphics[scale=0.31]{images/pitch_endproduct.png} 
\caption{Verhalten der Detektoraufnahme in Abhängigkeit des Parameters Pitch }
\label{img:pitch_detector}
\end{figure}

Der Startparamter Feature-Size verhält sich invers zum Parameter Pitch. Mit steigendem Feature-Size Wert, kommt es zu einer Erhöhung des Intensitätsmaximums und es werden weniger Frequenzen benötigt um die Gitterstruktur zu modellieren, somit sind auch weniger Peaks im Detektorsignal vorhanden(Abbildung \ref{img:fsize_detector}). 

\begin{figure}[hh]
\centering
\includegraphics[scale=0.31]{images/fsize_endproduct.png} 
\caption{Verhalten der Detektoraufnahme in Abhängigkeit des Parameters Fsize}
\label{img:fsize_detector}
\end{figure}

Der letzte Startparameter Sigma hat ansteigenden Wert ähnliche Auswirkungen auf die Detektoraufnahmen wie Fsize. Bei steigendem Sigma-Wert steigt das Intensitätsmaximum und die Anzahl der Peaks verringert sich (Abbildung \. Jedoch skaliert der Parameter Sigma anders als Fsize. Kleinere Unterschiede im Sigma Wert, kann größere Auswirkungen auf die Anzahl der Peaks haben. Auch der Anstieg des Intensitätsmaximums in Abhängigkeit zu Sigma ist geringer im Vergleich zur Abhängigkeit zu Fsize. 

\begin{figure}[hh]
\centering
\includegraphics[scale=0.28]{images/sigma_endproduct.png} 
\caption{Verhalten der Detektoraufnahme in Abhängigkeit des Parameters Sigma}
\label{img:sigma_detector}
\end{figure}

\chapter{Datenbasis}
\section{Generator}
Um einen Machine Learning Ansatz auszuführen, muss eine ausreichend große Datenbasis existieren. Dazu wurde im Rahmen dieser Arbeit ein  Generator entwickelt, welcher mit Hilfe der vorher beschriebenen Simulation ausreichend viele Trainingsdaten produzieren kann. Bevor der Generator beginnen kann, muss eine Population P von Startparametern festgelegt werden. Um die Berechung der Simulationsdaten, welche aus P resultieren, zu beschleunigen wurde die Simulation mit Hilfe von MPI (Message Parsing Interface) parallisiert. MPI dubliziert einen Algorithmus auf N beliebige Prozesse, welche dann mit Hilfe vordefinierter Routinen miteinander kommunizieren können \cite{MPI}. Mit Hilfe von MPI-Routinen wurde die Population auf N MPI generierte Prozesse aufgeteilt. Jeder Prozess berechnet dann für seine kleinere Population von Startparametern die Simulationsergebnisse, welche dann anschließend als Dateien in den Speicher geladen werden. Zusätzlich zum Simulationsergebnis wird die Population der Startparameter und die daraus generierten Gitterstrukturen gespeichert, welche als Labels für das Training des neuronalen Netzes vorgesehen sind. 

\begin{figure}[hh]
\centering
\includegraphics[scale=0.5]{images/generator_schema.png} 
\caption{Schematischer Aufbau des Generators}
\label{img:sigma_detector}
\end{figure}


\section{Wahl der Eingangsparameter}
\section{Training-, Validation- und Testdatensatz}

\chapter{Neuronales Netz}
\section{Architektur}
\section{Lossfunction}
\section{Optimizer}

\chapter{Ergebnisse und Diskussion}
	\section{Lernprozess}
	\section{Ergebnisse und statistische Auswertung}
	\section{Fazit}
	
\end{document}